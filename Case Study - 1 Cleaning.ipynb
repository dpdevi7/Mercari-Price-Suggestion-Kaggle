{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Case Study - 1 Cleaning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOP+2xI2ynhzDy3FFFv+HbF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqDOJV6HwBix","executionInfo":{"status":"ok","timestamp":1612940618919,"user_tz":-330,"elapsed":21298,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"3415985a-258c-431d-cd3a-43a79a24b4e0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yxtjs75owDWq","executionInfo":{"status":"ok","timestamp":1612940622541,"user_tz":-330,"elapsed":24909,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"5d4432a3-a5e7-4744-e0f5-53843f17f41a"},"source":["!pip install Unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\r\u001b[K     |█▍                              | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 5.2MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBv3b3s7wFYh","executionInfo":{"status":"ok","timestamp":1612940629246,"user_tz":-330,"elapsed":4551,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"d9e3b2fb-5429-4291-8535-726543737ab9"},"source":["train_df_path   = \"/content/drive/MyDrive/CASE STUDY - 1/submission/Factorization Machine/train.tsv\"\n","test2_df_path   = \"/content/drive/MyDrive/CASE STUDY - 1/submission/Factorization Machine/test_stg2.tsv\"\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from tqdm import tqdm\n","import regex as re\n","import unidecode\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import time\n","from nltk.stem import PorterStemmer \n","import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import OneHotEncoder, Normalizer, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import scipy\n","from scipy.sparse import csr_matrix\n","from nltk import word_tokenize, pos_tag, ne_chunk\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Ridge, LogisticRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","import tensorflow as tf\n","from tensorflow import keras\n","import itertools\n","import multiprocessing\n","from multiprocessing import Pool\n","from sklearn.model_selection import train_test_split\n","from scipy.sparse import hstack\n","import gc\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Input, GRU, LSTM, Dense, BatchNormalization, Dropout, RNN, Flatten, GlobalAveragePooling1D, concatenate, PReLU, Concatenate\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l1, l2, l1_l2\n","import pickle\n","\n","\n","num_cores = multiprocessing.cpu_count()\n","num_cores\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"XdS0IviewKvV"},"source":["train_df  = pd.read_table(train_df_path)\n","test_df = pd.read_table(test2_df_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGQu5I5AwMp6"},"source":["train_df['brand_name'].fillna(\"unknown\", inplace=True)\n","test_df['brand_name'].fillna(\"unknown\", inplace=True)\n","\n","train_df['category_name'].fillna(\"unknown/unknown/unknown\", inplace=True)\n","test_df['category_name'].fillna(\"unknown/unknown/unknown\", inplace=True)\n","\n","train_df['item_description'].fillna(\"no description yet\", inplace=True)\n","test_df['item_description'].fillna(\"no description yet\", inplace=True)\n","\n","train_df['cat_1'] = train_df.category_name.str.split('/').str.get(0).astype('category')\n","train_df['cat_2'] = train_df.category_name.str.split('/').str.get(1).astype('category')\n","train_df['cat_3'] = train_df.category_name.str.split('/').str.get(2).astype('category')\n","\n","test_df['cat_1'] = test_df.category_name.str.split('/').str.get(0).astype('category')\n","test_df['cat_2'] = test_df.category_name.str.split('/').str.get(1).astype('category')\n","test_df['cat_3'] = test_df.category_name.str.split('/').str.get(2).astype('category')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01153eHHwPVR"},"source":["global setofBrands\n","setofBrands = np.unique(train_df['brand_name'].astype(str).str.lower())\n","\n","\n","global setofCat1\n","setofCat1 = np.unique(train_df['cat_1'].astype(str).str.lower())\n","\n","global setofCat2\n","setofCat2 = np.unique(train_df['cat_2'].astype(str).str.lower())\n","\n","global setofCat3\n","setofCat3 = np.unique(train_df['cat_3'].astype(str).str.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2OFx7G6wRvA"},"source":["# https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\n","\n","# concat cat_1, cat_2, cat_3 \n","def concat_c1_c2_c3(c_1, c_2, c_3):\n","    return c_1 + \"/\" + c_2 + \"/\" + c_3\n","\n","\n","# filling missing cat_1 column from item_description and name\n","def miss_cat1(row):\n","    \n","    cat1 = row['cat_1']\n","    \n","    if row['cat_1']=='unknown':\n","        \n","        for i in setofCat1:\n","            if i in row['item_description'].lower():\n","                \n","                # print(\"1 desc {}\".format(i))\n","                \n","                cat1 = i\n","                break\n","            elif i in row['name'].lower():\n","                \n","                # print(\"1 name {}\".format(i))\n","                \n","                cat1 = i\n","                break   \n","    \n","    return cat1\n","\n","\n","## filling missing cat_2 column from item_description and name\n","def miss_cat2(row):\n","    \n","    cat2 = row['cat_2']\n","    \n","    if row['cat_2']=='unknown':\n","        \n","        for i in setofCat2:\n","            if i in row['item_description'].lower():\n","                \n","                # print(\"2 desc {}\".format(i))\n","                \n","                cat2 = i\n","                break\n","            elif i in row['name'].lower():\n","                \n","                # print(\"2 name {}\".format(i))\n","                \n","                cat2 = i\n","                break   \n","    \n","    return cat2\n","\n","\n","## filling missing cat_3 column from item_description and name\n","def miss_cat3(row):\n","    \n","    cat3 = row['cat_3']\n","    \n","    if row['cat_3']=='unknown':\n","        \n","        for i in setofCat3:\n","            if i in row['item_description'].lower():\n","                \n","                # print(\"3 desc {}\".format(i))\n","                \n","                cat3 = i\n","                break\n","            elif i in row['name'].lower():\n","                \n","                # print(\"3 name {}\".format(i))\n","                \n","                cat3 = i\n","                break   \n","    \n","    return cat3\n","    \n","\n","## get missing brand name from name and description\n","def miss_brand(row):\n","    \n","    brand_name = row['brand_name']\n","    desc = row['item_description'].lower()\n","    name = row['name'].lower()\n","    \n","    if row['brand_name']=='unknown':\n","        \n","        for i in setofBrands:\n","            if i in desc:\n","                                \n","                brand_name = i\n","                break\n","            elif i in name:\n","                \n","                brand_name = i\n","                break   \n","    \n","    return brand_name\n","\n","\n","def fill_missing_data(df):\n","    \n","    df['cat_1']         = df.apply(miss_cat1, axis=1)\n","    df['cat_2']         = df.apply(miss_cat2, axis=1)\n","    df['cat_3']         = df.apply(miss_cat3, axis=1)\n","    df['brand_name']    = df.apply(miss_brand, axis=1)\n","    df['category_name'] = df.apply(lambda x: concat_c1_c2_c3(str(x.cat_1), str(x.cat_2), str(x.cat_3)), axis=1)\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5epjF9ywV45"},"source":["\n","def parallelize_dataframe(df, func):\n","    df_split = np.array_split(df, num_cores)\n","    with Pool(num_cores) as pool:\n","        df = pd.concat(pool.map(func, df_split))\n","    return df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53sgvAePwXR4","executionInfo":{"status":"ok","timestamp":1612940938714,"user_tz":-330,"elapsed":248829,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"d53ddd1a-e0c6-40e6-90de-07d308a557e1"},"source":["%%time\n","train_df = parallelize_dataframe(train_df, fill_missing_data)\n","train_df.isnull().any()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 4.76 s, sys: 1.52 s, total: 6.28 s\n","Wall time: 3min 51s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdnFIXGawYib","executionInfo":{"status":"ok","timestamp":1612941477180,"user_tz":-330,"elapsed":787291,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"2cc576e1-adf4-4528-f28c-e20304bd7591"},"source":["%%time\n","test_df = parallelize_dataframe(test_df, fill_missing_data)\n","test_df.isnull().any()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 11.2 s, sys: 4.05 s, total: 15.3 s\n","Wall time: 8min 58s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nZJ5meH3waKz"},"source":["num_word_dict = {\n","    \n","    \"0\":\"zero\",\n","    \"1\":\"one\",\n","    \"2\":\"tow\",\n","    \"3\":\"three\",\n","    \"4\":\"four\",\n","    \"5\":\"five\",\n","    \"6\":\"six\",\n","    \"7\":\"seven\",\n","    \"8\":\"eight\",\n","    \"9\":\"nine\",\n","    '+':'plus'\n","}\n","\n","def number_to_word(text):\n","    sentence = ''\n","    for letter in text:\n","        try:\n","            \n","            if letter.isdigit():\n","                sentence = sentence + ' ' + num_word_dict[letter]\n","            else:\n","                sentence = sentence + letter\n","        except:\n","            sentence = sentence + ''\n","            \n","    return sentence\n","\n","def accented_to_english(text):\n","    return unidecode.unidecode(text)\n","\n","def fill_description(text):\n","    if len(text) == 0:\n","        return \"no description yet\"\n","    else:\n","        return text\n","\n","def fill_name(text):\n","    if len(text) == 0:\n","        return \"no name\"\n","    else:\n","        return text\n","    \n","def remove_stop_words(sent):\n","    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n","            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n","            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n","            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n","            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n","            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n","            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n","            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n","            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n","            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n","            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n","            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n","            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n","            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n","            'won', \"won't\", 'wouldn', \"wouldn't\"]\n","    \n","    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n","    return sent\n","\n","def decontracted(phrase):\n","    \n","    phrase = re.sub(r\"won't\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","\n","    \n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","    \n","    return phrase\n","\n","def remove_line(sent):\n","    sent = sent.replace('\\\\r', ' ')\n","    sent = sent.replace('\\\\\"', ' ')\n","    sent = sent.replace('\\\\n', ' ')\n","    \n","    return sent\n","\n","def setmming(sent):\n","    ps = PorterStemmer()\n","\n","def preprocess_cleaning(df):\n","    \n","    df['name'] = df['name'].apply(decontracted)\n","    df['name'] = df['name'].apply(remove_line)\n","    df['name'] = df['name'].apply(accented_to_english)\n","    df['name'] = df['name'].str.replace(r'[^A-Za-z0-9 ]', r' ')\n","    # df['name'] = df['name'].apply(number_to_word)\n","    df['name'] = df['name'].str.strip()\n","    df['name'] = df['name'].str.replace(' +', ' ')\n","    df['name'] = df['name'].str.lower()\n","    df['name'] = df['name'].apply(fill_name)\n","    \n","  \n","    df['brand_name'] = df['brand_name'].str.replace(r'[^A-Za-z0-9 ]', r' ')# remove all char except words and letters\n","    df['brand_name'] = df['brand_name'].str.strip()# removing leading and tailing spaces\n","    df['brand_name'] = df['brand_name'].str.replace(' +', ' ')# replacing double_space with single space\n","    df['brand_name'] = df['brand_name'].str.replace(' ', '_')# replacing space with under_score\n","    df['brand_name'] = df['brand_name'].str.lower()# converting to lowercase\n","    \n","\n","    df['cat_1'] = df['cat_1'].apply(accented_to_english)\n","    df['cat_1'] = df['cat_1'].str.replace(r'[^A-Za-z0-9 ]', r' ')# remove all char except words and letters\n","    df['cat_1'] = df['cat_1'].apply(remove_line)\n","    df['cat_1'] = df['cat_1'].str.strip()# removing leading and tailing spaces\n","    df['cat_1'] = df['cat_1'].str.replace(' +', ' ')# replacing double_space with single space\n","    df['cat_1'] = df['cat_1'].str.replace(' ', '_')# replacing space with under_score\n","    df['cat_1'] = df['cat_1'].str.lower()# converting to lowercase\n"," \n","    df['cat_2'] = df['cat_2'].apply(accented_to_english)\n","    df['cat_2'] = df['cat_2'].str.replace(r'[^A-Za-z0-9 ]', r' ')# remove all char except words and letters\n","    df['cat_2'] = df['cat_2'].str.strip()# removing leading and tailing spaces\n","    df['cat_2'] = df['cat_2'].str.replace(' +', ' ')# replacing double_space with single space\n","    df['cat_2'] = df['cat_2'].str.replace(' ', '_')# replacing space with under_score\n","    df['cat_2'] = df['cat_2'].str.lower()# converting to lowercase\n","    \n","\n","    df['cat_3'] = df['cat_3'].apply(accented_to_english)\n","    df['cat_3'] = df['cat_3'].str.replace(r'[^A-Za-z0-9 ]', r' ')# remove all char except words and letters\n","    df['cat_3'] = df['cat_3'].str.strip()# removing leading and tailing spaces\n","    df['cat_3'] = df['cat_3'].str.replace(' +', ' ')# replacing double_space with single space\n","    df['cat_3'] = df['cat_3'].str.replace(' ', '_')# replacing space with under_score\n","    df['cat_3'] = df['cat_3'].str.lower()# converting to lowercase\n","    \n","    df['category_name'] = df['cat_1'] + \"_\" + df['cat_2'] + \"_\" + df['cat_3']\n","    \n","    df['item_description'] = df['item_description'].apply(decontracted)\n","    df['item_description'] = df['item_description'].apply(remove_line)\n","    df['item_description'] = df['item_description'].apply(accented_to_english)\n","    df['item_description'] = df['item_description'].str.replace(r'[^a-zA-Z0-9 ]', r' ')# remove all char except words and letters\n","    df['item_description'] = df['item_description'].str.strip()# removing leading and tailing spaces\n","    df['item_description'] = df['item_description'].str.replace(' +', ' ')# replacing double_space with single space\n","    df['item_description'] = df['item_description'].str.lower()# converting to lowercase\n","    df['item_description'] = df['item_description'].apply(fill_description)\n","    \n","    return df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Wl7IQa6wbq5"},"source":["def parallelize_dataframe(df, func):\n","    df_split = np.array_split(df, num_cores)\n","    with Pool(num_cores) as pool:\n","        df = pd.concat(pool.map(func, df_split))\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CuOR97NvwdCu","executionInfo":{"status":"ok","timestamp":1612941658792,"user_tz":-330,"elapsed":959229,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"aa3f35bd-e901-422a-e013-362030f60f7f"},"source":["%%time\n","train_df = parallelize_dataframe(train_df, preprocess_cleaning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 6.8 s, sys: 2.29 s, total: 9.08 s\n","Wall time: 3min\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QXB7Ccjn5qWu"},"source":["\n","train_test_save_path = \"/content/drive/MyDrive/CASE STUDY - 1/data/\"\n","\n","train_df.to_csv(train_test_save_path+\"train__cleaned.csv\", index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbIEU3NK5sDW"},"source":["del train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WH70-6Rj6MQW","executionInfo":{"status":"ok","timestamp":1612943256351,"user_tz":-330,"elapsed":4198,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"7fa2df49-0c11-46a6-8b13-5115642177c1"},"source":["gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["472"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6xQz-4mwed3","executionInfo":{"status":"ok","timestamp":1612943688183,"user_tz":-330,"elapsed":430110,"user":{"displayName":"Devi Prasad","photoUrl":"","userId":"17542284671188671364"}},"outputId":"c6ff1c9d-fa77-4e52-8487-02dc7e17ca4c"},"source":["%%time\n","test_df = parallelize_dataframe(test_df, preprocess_cleaning)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 15.5 s, sys: 7.03 s, total: 22.5 s\n","Wall time: 7min 8s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MyQXYn7Kwf-4"},"source":["\n","train_test_save_path = \"/content/drive/MyDrive/CASE STUDY - 1/data/\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FQFJmw0wu5D"},"source":["\n","train_test_save_path = \"/content/drive/MyDrive/CASE STUDY - 1/data/\"\n","\n","test_df.to_csv(train_test_save_path+\"test__cleaned.csv\", index=False)\n"],"execution_count":null,"outputs":[]}]}